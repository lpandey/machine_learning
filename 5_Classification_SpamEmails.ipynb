{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was first exposed to this exercise in [Andrew Ng's Intro to Machine Learning class on Coursera](https://www.coursera.org/learn/machine-learning). I revisited it through end-of-chapter-3 exercise in [Aurélien Géron's Machine Learning Handbook](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) in order to extend my understanding of the concepts as well as the relevant python tools. I am using Aurélien's code on [github](https://github.com/ageron/handson-ml2) with some tweaks for 20050311_spam_2 and 20030228_hard_ham data from [spamassassin corpus](http://spamassassin.apache.org/old/publiccorpus/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup environment\n",
    "import os\n",
    "import urllib\n",
    "import tarfile\n",
    "import email\n",
    "import email.policy\n",
    "import nltk\n",
    "import urlextract\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get email data from spamAssassin public corpus\n",
    "ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "SPAM_URL = \"\".join([ROOT, \"20050311_spam_2.tar.bz2\"])\n",
    "HAM_URL = \"\".join([ROOT, \"20030228_hard_ham.tar.bz2\"])\n",
    "DATA_PATH = os.path.join(\"data\", \"spamClf\")\n",
    "\n",
    "# define function to pull and extract data\n",
    "def getSpamData(dataUrl=SPAM_URL, dataPath=DATA_PATH):\n",
    "    '''pull and extract spamAssasin email data'''\n",
    "    if not os.path.isdir(dataPath):\n",
    "        os.makedirs(dataPath)\n",
    "    for f, url in [[\"spam_2.tar.bz2\", SPAM_URL], [\"hard_ham.tar.bz2\", HAM_URL]]:\n",
    "        path = os.path.join(dataPath, f)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "            tar_bz2_f = tarfile.open(path)\n",
    "            tar_bz2_f.extractall(path=DATA_PATH)\n",
    "            tar_bz2_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSpamData(dataUrl=SPAM_URL, dataPath=DATA_PATH) # call function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data (emails)\n",
    "SPAM_DIR = os.path.join(DATA_PATH, \"spam_2\")\n",
    "HAM_DIR = os.path.join(DATA_PATH, \"hard_ham\")\n",
    "\n",
    "# filenames are 38 characters\n",
    "spam_files = [f for f in sorted(os.listdir(SPAM_DIR)) if len(f) >= 35]\n",
    "ham_files = [f for f in sorted(os.listdir(HAM_DIR)) if len(f) >= 35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam files: 1396\n",
      "Number of ham files: 250\n"
     ]
    }
   ],
   "source": [
    "# number of pulled spam and ham files\n",
    "print(f\"Number of spam files: {len(spam_files)}\")\n",
    "print(f\"Number of ham files: {len(ham_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This (spam_2 and hard_ham) set of data a considerably larger ratio (1396/250) of spam file over ham files. The \"spam\" and \"easy_ham\" sets had this ratio at 500/2500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load emails (create email parser instance)\n",
    "def loadEmail(isSpam, file, spamPath=DATA_PATH):\n",
    "    if isSpam:\n",
    "        folder = \"spam_2\"\n",
    "    else:\n",
    "        folder = \"hard_ham\"\n",
    "    with open(os.path.join(spamPath, folder, file), \"rb\") as f:\n",
    "        # parser API used (vs. feedparser) as emails in files (not livefeed)\n",
    "        # create BytesParser instance\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_emails = [loadEmail(isSpam=True, file=f) for f in spam_files]\n",
    "ham_emails = [loadEmail(isSpam=False, file=f) for f in ham_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get email structures\n",
    "def getEmailStructure(email):\n",
    "    '''get structure of email'''\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload() # list of multipart message objects\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            getEmailStructure(subEmail) for subEmail in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type() # email's content type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test sets\n",
    "\n",
    "# list of email objects\n",
    "X = np.array(spam_emails + ham_emails, dtype=object)\n",
    "\n",
    "# target labels\n",
    "y = np.array([1] * len(spam_emails) + [0] * len(ham_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am trying to secure three of four virtual hostnames on our Apache server.\n",
      "We are not taking credit card orders or user's personal information, but are\n",
      "merely hoping to secure email and calendar web transactions for our users.\n",
      "We are not running any secure applications on the root host.\n",
      "\n",
      "I have been testing this week with CA, client, and host certificate\n",
      "requests, certificates, and keys, and think I have a fairly good beginner's\n",
      "grasp of the commands and command line options.\n",
      "\n",
      "\n",
      "My questions are:\n",
      "\n",
      "1.  Is it necessary to create a CA certificate for each of the secure\n",
      "virtual hosts, or can one CA certificate for the root be used to sign each\n",
      "of the keys for all three common names we are trying to secure?\n",
      "\n",
      "2.  Even though the root host is not conducting secure transactions, am I\n",
      "correct in configuring the server with a CACertificateFile in the main body\n",
      "of httpsd.conf and then setting the CACertificateFile for each virtual host\n",
      "in the <Virtual . . .> section of httpsd.conf?  This sort of assumes the\n",
      "answer to 1. is  - you need a CA for each virtual host.\n",
      "\n",
      "3.  Is it necessary to create a client certificate to distribute to our\n",
      "users, or is it sufficient to have the CA certificate and a server\n",
      "certificate for the virtual hosts?  Wouldn't a client certificate only be\n",
      "necessary if we were trying to verify the client's identity?  Would that be\n",
      "a good idea given our scenario?\n",
      "\n",
      "Thanks in advance for your help.\n",
      "\n",
      "\n",
      "______________________________________________________________________\n",
      "OpenSSL Project                                 http://www.openssl.org\n",
      "User Support Mailing List                    openssl-users@openssl.org\n",
      "Automated List Manager                           majordomo@openssl.org\n"
     ]
    }
   ],
   "source": [
    "# print first ham sample from X_train \n",
    "print(X_train[y_train==0][3].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess ...\n",
    "\n",
    "# function to converst email body into bag of words\n",
    "def html2text(html):\n",
    "    soup = BeautifulSoup(html, features=\"lxml\")\n",
    "    text = soup.get_text() # remove html markups\n",
    "    \n",
    "    if soup.head: soup.head.decompose() # remove headers\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        a.replace_with(\" HYPERLINK \") # convert all <a> tags with text HYPERLINK\n",
    "    for s in soup([\"script\", \"style\"]):\n",
    "        s.decompose() # remove tags\n",
    "    text = ' '.join(soup.stripped_strings) # retrieve tag contents\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><body bgColor=\"#CCCCCC\" topmargin=1 onMouseOver=\"window.status=''; return true\" oncontextmenu=\"return false\" ondragstart=\"return false\" onselectstart=\"return false\">\n",
      "<div align=\"center\">Hello, jm@netnoteinc.com<BR><BR></div><div align=\"center\"></div><p align=\"center\"><b><font face=\"Arial\" size=\"4\">Human Growth Hormone Therapy</font></b></p>\n",
      "<p align=\"center\"><b><font face=\"Arial\" size=\"4\">Lose weight while building lean muscle mass<br>and reversing the ravages of aging all at once.</font>< ...\n"
     ]
    }
   ],
   "source": [
    "# check html2text works - print spam html\n",
    "htmlSpamEmails = [email for email in X_train[y_train==1]\n",
    "                 if getEmailStructure(email) == \"text/html\"]\n",
    "\n",
    "sampleHtmlSpam = htmlSpamEmails[10]\n",
    "# first 500 characters\n",
    "print(sampleHtmlSpam.get_content().strip()[:500], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, jm@netnoteinc.com Human Growth Hormone Therapy Lose weight while building lean muscle mass and reversing the ravages of aging all at once. Remarkable discoveries about Human Growth Hormones ( HGH ) are changing the way we think about aging and weight loss. Lose Weight Build Muscle Tone Reverse Aging Increased Libido Duration Of Penile Erection Healthier Bones Improved Memory Improved skin New Hair Growth Wrinkle Disappearance HYPERLINK You are receiving this email as a subscr iber to the  ...\n"
     ]
    }
   ],
   "source": [
    "# check html2text works - print spam text (using html2text function)\n",
    "print(html2text(sampleHtmlSpam.get_content())[:500], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, jm@netnoteinc.com Human Growth Hormone Therapy Lose weight while building lean muscle mass and reversing the ravages of aging all at once. Remarkable discoveries about Human Growth Hormones ( HGH ) are changing the way we think about aging and weight loss. Lose Weight Build Muscle Tone Reverse Aging Increased Libido Duration Of Penile Erection Healthier Bones Improved Memory Improved skin New Hair Growth Wrinkle Disappearance HYPERLINK You are receiving this email as a subscr iber to the  ...\n"
     ]
    }
   ],
   "source": [
    "# check html2text works - print spam text (using html2text function)\n",
    "print(html2text(sampleHtmlSpam.get_content())[:500], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess ...\n",
    "# function to convert any content to plain text\n",
    "def email2text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except:\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html2text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, jm@netnoteinc.com Human Growth Hormone Therapy Lose weight while building lean muscle mass an ...\n"
     ]
    }
   ],
   "source": [
    "print(email2text(sampleHtmlSpam)[:100], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess ...\n",
    "stemmer = nltk.PorterStemmer() # initialize stemmer\n",
    "url_extractor = urlextract.URLExtract() # initialize url extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess ...\n",
    "# class to convert emails to word counters\n",
    "#nltk.download(\"stopwords\")\n",
    "\n",
    "class Emails2WordCounts(BaseEstimator, TransformerMixin):\n",
    "    '''convert emails to word counters'''\n",
    "    def __init__(self, strip_headers=True, replace_urls=True,\n",
    "                 replace_numbers=True, remove_punctuation=True,\n",
    "                 stemming=True, lowercase=True,\n",
    "                remove_stopwords=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.stemming = stemming\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_stopwords=remove_stopwords\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email2text(email) or ''\n",
    "            if self.lowercase:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "            for url in urls:\n",
    "                text = text.replace(url, \"URL\")\n",
    "            if self.replace_numbers:\n",
    "                regx = re.compile(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?')\n",
    "                text = regx.sub(string=text, repl=\"NUMBER\")\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "                regx = re.compile(r\"([^\\w\\s]+)|([_-]+)\")\n",
    "                text = regx.sub(string=text, repl=\" \")\n",
    "            if self.remove_stopwords:\n",
    "                words = text.split()\n",
    "                keepWords = [word for word in words if \n",
    "                             word not in stopwords.words('english')] \n",
    "            word_counts = Counter(keepWords)\n",
    "            if self.stemming:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'dnumber': 43, 'bnumber': 38, 'anumb': 29, 'number': 24, 'cnumber': 21, 'url': 14, 'free': 12, 'get': 9, 'cb': 9, 'account': 8, 'ca': 8, 'fnumber': 8, 'download': 7, 'instal': 7, 'softwar': 7, 'open': 7, 'bc': 7, 'ba': 7, 'cd': 6, 'bb': 6, 'purchas': 5, 'fe': 5, 'ac': 5, 'enumb': 5, 'al': 4, 'bonu': 4, 'ce': 4, 'sign': 3, 'ee': 3, 'bf': 3, 'cc': 3, 'cf': 3, 'email': 2, 'member': 2, 'remov': 2, 'l': 2, 'real': 2, 'requir': 2, 'de': 2, 'da': 2, 'dd': 2, 'ab': 2, 'bd': 2, 'df': 2, 'spam': 1, 'list': 1, 'topdollaremail': 1, 'opt': 1, 'servic': 1, 'see': 1, 'receiv': 1, 'easi': 1, 'rea': 1, 'buy': 1, 'extra': 1, 'paypal': 1, 'first': 1, 'purch': 1, 'ase': 1, 'non': 1, 'paid': 1, 'futur': 1, 'mail': 1, 'pleas': 1, 'unsubscrib': 1, 'click': 1, 'user': 1, 'info': 1, 'scroll': 1, 'bottom': 1, 'page': 1, 'lose': 1, 'referr': 1, 'money': 1, 'owe': 1, 'enumbersmtp': 1, 'eb': 1, 'anumberpc': 1, 'anumbersmtp': 1, 'ec': 1, 'fb': 1, 'fd': 1, 'aa': 1, 'ed': 1, 'http': 1, 'www': 1, 'seek': 1}),\n",
       "       Counter({'phone': 3, 'url': 3, 'hyperlink': 2, 'spamassassin': 2, 'sight': 2, 'list': 2, 'say': 1, 'goodby': 1, 'drop': 1, 'call': 1, 'forev': 1, 'new': 1, 'technolog': 1, 'breakthrough': 1, 'cell': 1, 'pc': 1, 'cordless': 1, 'special': 1, 'member': 1, 'deal': 1, 'qdaaobw': 1, 'deathtospamdeathtospamdeathtospam': 1, 'email': 1, 'sponsor': 1, 'jabber': 1, 'world': 1, 'fastest': 1, 'grow': 1, 'real': 1, 'time': 1, 'commun': 1, 'platform': 1, 'im': 1, 'build': 1, 'mail': 1, 'sourceforg': 1, 'net': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check it for a sample email\n",
    "X_few = X_train[:2]\n",
    "X_few_wc = Emails2WordCounts().fit_transform(X_few)\n",
    "X_few_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess ...\n",
    "# build list (orderd) of most common words and convert it to vectors\n",
    "class WordCounts2Vector(BaseEstimator, TransformerMixin):\n",
    "    '''convert word counts to vectors'''\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X: # counter dictionary object\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in \n",
    "                            enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), \n",
    "                          shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x11 sparse matrix of type '<class 'numpy.longlong'>'\n",
       "\twith 13 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounts2Vector(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wc)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[184,  14,  24,  12,  38,  43,  21,  29,   9,   9,   8],\n",
       "       [ 42,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()\n",
    "# first column represents words in that row(email) that \n",
    "#     do not appear in the vocabulary\n",
    "# the rest of the columns represent the times the vocabulary\n",
    "#     words appear in the email\n",
    "# the vocabulary is obtained using the vocabulary_ \n",
    "#     attribute of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 1,\n",
       " 'number': 2,\n",
       " 'free': 3,\n",
       " 'bnumber': 4,\n",
       " 'dnumber': 5,\n",
       " 'cnumber': 6,\n",
       " 'anumb': 7,\n",
       " 'get': 8,\n",
       " 'cb': 9,\n",
       " 'account': 10}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min, sys: 24.8 s, total: 2min 25s\n",
      "Wall time: 3min 57s\n"
     ]
    }
   ],
   "source": [
    "# training pipeline\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email2wordcounts\", Emails2WordCounts()),\n",
    "    (\"wordcounts2vector\", WordCounts2Vector()),\n",
    "])\n",
    "\n",
    "%time X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.954, total=   0.2s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.952, total=   0.3s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.952, total=   0.3s\n",
      "\n",
      "Mean cross validation score: 0.9529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"lbfgs\", \n",
    "                             max_iter=1000, random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed,\n",
    "                        y_train, cv=3, verbose=3)\n",
    "\n",
    "print(f\"\\nMean cross validation score: {score.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.3 s, sys: 7.34 s, total: 41.7 s\n",
      "Wall time: 1min 8s\n",
      "\n",
      "Precision: 97.60%\n",
      "Recall: 98.62%\n"
     ]
    }
   ],
   "source": [
    "%time X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000,\n",
    "                             random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"\\nPrecision: {:.2f}%\".format(\n",
    "    100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model gives reasonably high Precision and Recall scores on the test set. Lets check the performance using sklearn SVM (SVC) with GridSearch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator\n",
    "svm_clf = SVC()\n",
    "\n",
    "# grid search parameters\n",
    "param_grid = {\n",
    "    'C': np.logspace(-1, 2, 10), # 0.1-100\n",
    "    'gamma': np.logspace(-1, 1, 10), # 0.1-10\n",
    "    'kernel': [\"linear\", \"rbf\"]\n",
    "}\n",
    "\n",
    "# perform grid\n",
    "grid_search = GridSearchCV(svm_clf, param_grid, cv=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 200 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 39s, sys: 807 ms, total: 3min 40s\n",
      "Wall time: 3min 43s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=SVC(),\n",
       "             param_grid={'C': array([  0.1       ,   0.21544347,   0.46415888,   1.        ,\n",
       "         2.15443469,   4.64158883,  10.        ,  21.5443469 ,\n",
       "        46.41588834, 100.        ]),\n",
       "                         'gamma': array([ 0.1       ,  0.16681005,  0.27825594,  0.46415888,  0.77426368,\n",
       "        1.29154967,  2.15443469,  3.59381366,  5.9948425 , 10.        ]),\n",
       "                         'kernel': ['linear', 'rbf']},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit best model\n",
    "%time grid_search.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9506055342327069\n",
      "{'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# print best parameters and cross validation score\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 98.26%\n",
      "Recall: 97.92%\n"
     ]
    }
   ],
   "source": [
    "# evaluate metrics on the test set\n",
    "y_pred = grid_search.predict(X_test_transformed)\n",
    "\n",
    "print(\"\\nPrecision: {:.2f}%\".format(\n",
    "    100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluated metrics on the test set using the grid search method on svm classifier were comparable to the logistic regression classifier. Both pipelines gave precision and recall scores above 97.5%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
